{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile test.py\n",
    "#print('hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "USE_FINETUNE = True     \n",
    "FOLDS = 5\n",
    "SEED = 66\n",
    "INPUTPATH = '../../input'\n",
    "NUM_EPOCH = 100\n",
    "BATCH_SIZE = 16384\n",
    "LR = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MDL_PATH  = '../models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train = pd.read_csv(f'{INPUTPATH}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train.to_parquet(f'{INPUTPATH}/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.35 s, sys: 3.92 s, total: 12.3 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_parquet(f'{INPUTPATH}/train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1571415, 139)\n",
      "CPU times: user 1.89 s, sys: 741 ms, total: 2.64 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "print(train.shape)\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "train['action'] =  \\\n",
    "(  (train['resp_1'] > 0.00001 ) & \\\n",
    "   (train['resp_2'] > 0.00001 ) & \\\n",
    "   (train['resp_3'] > 0.00001 ) & \\\n",
    "   (train['resp_4'] > 0.00001 ) & \\\n",
    "   (train['resp'] > 0.00001 )   ).astype('int')\n",
    "\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T\n",
    "#X = cp.array(train[features].values)\n",
    "#y = cp.array(np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T) #Multitarget\n",
    "\n",
    "#f_mean = np.mean(train[features[1:]].values,axis=0)\n",
    "\n",
    "f_mean = np.load( f'{INPUTPATH}/f_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 ms, sys: 15 µs, total: 1.17 ms\n",
      "Wall time: 805 µs\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#np.save( f'{INPUTPATH}/f_mean.npy',f_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f'{INPUTPATH}/example_test.csv')\n",
    "pred_df  = pd.read_csv(f'{INPUTPATH}/example_sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[-1])\n",
    "print(y.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Gaussian noise regularizer.\n",
    "\n",
    "    Args:\n",
    "        sigma (float, optional): relative standard deviation used to generate the\n",
    "            noise. Relative means that it will be multiplied by the magnitude of\n",
    "            the value your are adding the noise to. This means that sigma can be\n",
    "            the same regardless of the scale of the vector.\n",
    "        is_relative_detach (bool, optional): whether to detach the variable before\n",
    "            computing the scale of the noise. If `False` then the scale of the noise\n",
    "            won't be seen as a constant but something to optimize: this will bias the\n",
    "            network to generate vectors with smaller values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.1, is_relative_detach=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.is_relative_detach = is_relative_detach\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.noise = torch.tensor(0).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma != 0:\n",
    "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
    "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
    "            x = x + sampled_noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/simple_autoencoder.py\n",
    "#https://discuss.pytorch.org/t/pytorch-equivalent-of-keras/29412/2\n",
    "class autoencoder(nn.Module):\n",
    "    '''\n",
    "    >> model = \n",
    "        autoencoder(input_size = X.shape[-1], output_size = y.shape[-1],\\\n",
    "        noise = 0.1).to(DEVICE)\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(autoencoder, self).__init__()\n",
    "        input_size = kwargs['input_size']\n",
    "        output_size = kwargs['output_size']\n",
    "        noise = kwargs['noise']\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_size),\n",
    "            #GaussianNoise(noise),\n",
    "            nn.Linear(input_size, 640),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(640, input_size)\n",
    "        )\n",
    "        self.hidden = nn.Linear(input_size, 320)\n",
    "        self.bat = nn.BatchNorm1d(320)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.hidden2 = nn.Linear(320, output_size)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.bat(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet (nn.Module):\n",
    "    '''\n",
    "    >> model = \n",
    "        MLPNet(input_size = X.shape[-1], output_size = y.shape[-1]).to(DEVICE)\n",
    "    '''\n",
    "    def __init__(self,  **kwargs):\n",
    "        super(MLPNet, self).__init__()\n",
    "        input_size = kwargs['input_size']\n",
    "        output_size = kwargs['output_size']\n",
    "        self.fc1 = nn.Linear(input_size, 512)   \n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, output_size)\n",
    "        self.dropout1 = nn.Dropout2d(0.2)\n",
    "        self.dropout2 = nn.Dropout2d(0.2)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.act(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, dataset, target):\n",
    "        self.dataset = dataset\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            'x': torch.tensor(self.dataset[item, :], dtype=torch.float),\n",
    "            'y': torch.tensor(self.target[item, :], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder(input_size = X.shape[-1], output_size = y.shape[-1], noise=0.1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=LR, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5324de7f748455e8c9acb86c1a85a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.2452\n",
      "epoch [2/100], loss:0.2452\n",
      "epoch [3/100], loss:0.2453\n",
      "epoch [4/100], loss:0.2453\n",
      "epoch [5/100], loss:0.2477\n",
      "epoch [6/100], loss:0.2457\n",
      "epoch [7/100], loss:0.2453\n",
      "epoch [8/100], loss:0.2451\n",
      "epoch [9/100], loss:0.2450\n",
      "epoch [10/100], loss:0.2454\n",
      "epoch [11/100], loss:0.2453\n",
      "epoch [12/100], loss:0.2452\n",
      "epoch [13/100], loss:0.2450\n",
      "epoch [14/100], loss:0.2452\n",
      "epoch [15/100], loss:0.2451\n",
      "epoch [16/100], loss:0.2452\n",
      "epoch [17/100], loss:0.2450\n",
      "epoch [18/100], loss:0.2449\n",
      "epoch [19/100], loss:0.2450\n",
      "epoch [20/100], loss:0.2448\n",
      "epoch [21/100], loss:0.2447\n",
      "epoch [22/100], loss:0.2447\n",
      "epoch [23/100], loss:0.2446\n",
      "epoch [24/100], loss:0.2445\n",
      "epoch [25/100], loss:0.2444\n",
      "epoch [26/100], loss:0.2444\n",
      "epoch [27/100], loss:0.2443\n",
      "epoch [28/100], loss:0.2442\n",
      "epoch [29/100], loss:0.2442\n",
      "epoch [30/100], loss:0.2442\n",
      "epoch [31/100], loss:0.2440\n",
      "epoch [32/100], loss:0.2441\n",
      "epoch [33/100], loss:0.2440\n",
      "epoch [34/100], loss:0.2442\n",
      "epoch [35/100], loss:0.2439\n",
      "epoch [36/100], loss:0.2437\n",
      "epoch [37/100], loss:0.2438\n",
      "epoch [38/100], loss:0.2436\n",
      "epoch [39/100], loss:0.2435\n",
      "epoch [40/100], loss:0.2434\n",
      "epoch [41/100], loss:0.2435\n",
      "epoch [42/100], loss:0.2436\n",
      "epoch [43/100], loss:0.2437\n",
      "epoch [44/100], loss:0.2434\n",
      "epoch [45/100], loss:0.2435\n",
      "epoch [46/100], loss:0.2431\n",
      "epoch [47/100], loss:0.2430\n",
      "epoch [48/100], loss:0.2427\n",
      "epoch [49/100], loss:0.2429\n",
      "epoch [50/100], loss:0.2425\n",
      "epoch [51/100], loss:0.2427\n",
      "epoch [52/100], loss:0.2425\n",
      "epoch [53/100], loss:0.2423\n",
      "epoch [54/100], loss:0.2424\n",
      "epoch [55/100], loss:0.2419\n",
      "epoch [56/100], loss:0.2422\n",
      "epoch [57/100], loss:0.2421\n",
      "epoch [58/100], loss:0.2421\n",
      "epoch [59/100], loss:0.2418\n",
      "epoch [60/100], loss:0.2414\n",
      "epoch [61/100], loss:0.2416\n",
      "epoch [62/100], loss:0.2414\n",
      "epoch [63/100], loss:0.2415\n",
      "epoch [64/100], loss:0.2414\n",
      "epoch [65/100], loss:0.2417\n",
      "epoch [66/100], loss:0.2413\n",
      "epoch [67/100], loss:0.2414\n",
      "epoch [68/100], loss:0.2416\n",
      "epoch [69/100], loss:0.2411\n",
      "epoch [70/100], loss:0.2409\n",
      "epoch [71/100], loss:0.2412\n",
      "epoch [72/100], loss:0.2411\n",
      "epoch [73/100], loss:0.2412\n",
      "epoch [74/100], loss:0.2412\n",
      "epoch [75/100], loss:0.2410\n",
      "epoch [76/100], loss:0.2406\n",
      "epoch [77/100], loss:0.2413\n",
      "epoch [78/100], loss:0.2411\n",
      "epoch [79/100], loss:0.2406\n",
      "epoch [80/100], loss:0.2411\n",
      "epoch [81/100], loss:0.2407\n",
      "epoch [82/100], loss:0.2407\n",
      "epoch [83/100], loss:0.2405\n",
      "epoch [84/100], loss:0.2411\n",
      "epoch [85/100], loss:0.2405\n",
      "epoch [86/100], loss:0.2400\n",
      "epoch [87/100], loss:0.2401\n",
      "epoch [88/100], loss:0.2401\n",
      "epoch [89/100], loss:0.2407\n",
      "epoch [90/100], loss:0.2401\n",
      "epoch [91/100], loss:0.2412\n",
      "epoch [92/100], loss:0.2413\n",
      "epoch [93/100], loss:0.2407\n",
      "epoch [94/100], loss:0.2417\n",
      "epoch [95/100], loss:0.2414\n",
      "epoch [96/100], loss:0.2414\n",
      "epoch [97/100], loss:0.2418\n",
      "epoch [98/100], loss:0.2418\n",
      "epoch [99/100], loss:0.2419\n",
      "epoch [100/100], loss:0.2415\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(NUM_EPOCH)):\n",
    "    for data in dataloader:\n",
    "        x = data['x'].to(DEVICE)\n",
    "        y = data['y'].to(DEVICE)\n",
    "        # ===================forward=====================\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, NUM_EPOCH, loss.data.to('cpu').detach().numpy().copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDL_NAME = 'autoencoder'\n",
    "VER = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{MDL_PATH}/{MDL_NAME}_{VER}'):\n",
    "    os.mkdir(f'{MDL_PATH}/{MDL_NAME}_{VER}')\n",
    "save_path = f'{MDL_PATH}/{MDL_NAME}_{VER}/{MDL_NAME}_'+str(epoch)+'.pth'\n",
    "torch.save(model.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 0.5\n",
    "x_tt = test_df.loc[:, features].values\n",
    "if np.isnan(x_tt[:, 1:].sum()):\n",
    "    x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "pred = 0.0\n",
    "X_test = torch.FloatTensor(x_tt).to(DEVICE)\n",
    "pred = model(X_test).cpu().detach().numpy()\n",
    "pred_df.action = np.where(pred >= th, 1, 0).astype(int) ## 5つの予測値をどうするか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/autoencoder_test\n"
     ]
    }
   ],
   "source": [
    "print(f'{MDL_PATH}/{MDL_NAME}_{VER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: ../models/autoencoder_test/dataset-metadata.json\r\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p ../models/autoencoder_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "{\n",
    "    \"title\": \"Jane-Street\",\n",
    "    \"id\": \"shinsei66/Jane-Street\",\n",
    "    \"subtitle\": \"\",\n",
    "    \"description\": \"\",\n",
    "    \"isPrivate\": true,\n",
    "    \"licenses\": [\n",
    "        {\n",
    "            \"name\": \"unknown\" ##Invalid value for `license_name` (MIT), must be one of ['CC0-1.0', 'CC-BY-SA-4.0', 'GPL-2.0', 'ODbL-1.0', 'CC-BY-NC-SA-4.0', 'unknown', 'DbCL-1.0', 'CC-BY-SA-3.0', 'copyright-authors', 'other', 'reddit-api', 'world-bank']\n",
    "1\n",
    "\n",
    "        }\n",
    "    ],\n",
    "    \"keywords\": [],\n",
    "    \"collaborators\": [],\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"description\": null,\n",
    "            \"name\": \"autoencoder_99.pth\",\n",
    "            \"totalBytes\": 848,\n",
    "            \"columns\": []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file autoencoder_99.pth\n",
      "100%|████████████████████████████████████████| 836k/836k [08:47<00:00, 1.62kB/s]\n",
      "Upload successful: autoencoder_99.pth (836KB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/shinsei66/Jane-Street\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p  ../models/autoencoder_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
